#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ORQUESTADOR DE DATOS √ìPTIMO - SISTEMA UNIFICADO QBTC
Script orquestador en ASCII puro que automatiza la captura de todos los datos requeridos.
"""

import asyncio
import aiohttp
import json
import sqlite3
import logging
import time
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

# CONSTANTES F√çSICAS REALES
PHYSICAL_CONSTANTS = {
    "QUANTUM_COHERENCE": 0.75,
    "QUANTUM_CONSCIOUSNESS": 0.8,
    "QUANTUM_ENTANGLEMENT": 0.65,
    "QUANTUM_SUPERPOSITION": 0.7,
    "QUANTUM_TUNNELING": 0.6,
    "MARKET_VOLATILITY": 0.05,
    "MARKET_MOMENTUM": 0.1,
    "MARKET_LIQUIDITY": 0.75,
    "MARKET_SPREAD": 0.001,
    "MARKET_DEPTH": 500000,
    "FUNDING_RATE": 0.02,
    "FUNDING_VOLATILITY": 0.01,
    "FUNDING_DEVIATION": 0.5,
    "FUNDING_ANNUALIZED": 5,
    "LIQUIDATION_PROBABILITY": 0.05,
    "SLIPPAGE_RATE": 0.0025,
    "VOLATILITY_RISK": 0.1,
    "EXECUTION_RISK": 0.005,
    "VOLUME_24H": 500000,
    "VOLUME_RATIO": 0.75,
    "VOLUME_EXPANSION": 300000,
    "PRICE_CHANGE": 0.02,
    "PRICE_ACCELERATION": 0.015,
    "PRICE_MOMENTUM": 0.01,
    "TIME_TO_FUNDING": 1800000,
    "SESSION_INTENSITY": 0.6,
    "TEMPORAL_RESONANCE": 0.7,
    "FIBONACCI_STRENGTH": 0.75,
    "FIBONACCI_INDEX": 5,
    "NEURAL_CONFIDENCE": 0.85,
    "NEURAL_COHERENCE": 0.8,
    "NEURAL_ENTANGLEMENT": 0.7,
    "BASE_LEVERAGE": 15,
    "CONSERVATIVE_LEVERAGE": 10,
    "AGGRESSIVE_LEVERAGE": 25,
    "STOP_LOSS": 0.03,
    "TAKE_PROFIT": 0.06,
    "BASE_SCORE": 0.65,
    "CONFIDENCE_SCORE": 0.75,
    "QUALITY_SCORE": 0.8
}

# CONFIGURACI√ìN DE LOGGING
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('orquestador_datos.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DataPriority(Enum):
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4

@dataclass
class DataRequirement:
    name: str
    priority: DataPriority
    endpoint: str
    frequency_seconds: int
    last_captured: datetime = None
    is_active: bool = True

# DEFINICI√ìN DE REQUERIMIENTOS DE DATOS
DATA_REQUIREMENTS = [
    # DATOS CR√çTICOS (PRIORIDAD 1)
    DataRequirement("spot_market_data", DataPriority.CRITICAL, "/api/spot-data", 30),
    DataRequirement("futures_market_data", DataPriority.CRITICAL, "/api/futures-data", 30),
    DataRequirement("quantum_metrics_basic", DataPriority.CRITICAL, "/api/quantum-metrics", 45),
    DataRequirement("system_health", DataPriority.CRITICAL, "/health", 60),
    
    # DATOS AVANZADOS (PRIORIDAD 2)
    DataRequirement("srona_options_data", DataPriority.HIGH, "/api/options-data", 60),
    DataRequirement("technical_analysis", DataPriority.HIGH, "/api/technical-analysis", 90),
    DataRequirement("smart_money_data", DataPriority.HIGH, "/api/smart-money", 120),
    DataRequirement("sentiment_data", DataPriority.HIGH, "/api/sentiment", 90),
    
    # DATOS CU√ÅNTICOS (PRIORIDAD 3)
    DataRequirement("quantum_factors_advanced", DataPriority.MEDIUM, "/api/quantum-factors", 120),
    DataRequirement("neural_metrics", DataPriority.MEDIUM, "/api/neural-context", 150),
    DataRequirement("leonardo_feynman_analysis", DataPriority.MEDIUM, "/api/leonardo-feynman", 180),
    
    # DATOS DE SISTEMA (PRIORIDAD 4)
    DataRequirement("vpn_config", DataPriority.LOW, "/api/vpn-status", 300),
    DataRequirement("historical_logs", DataPriority.LOW, "/api/logs", 600),
    DataRequirement("frontend_data", DataPriority.LOW, "/api/status", 120)
]

# CONFIGURACI√ìN DE ENDPOINTS
ENDPOINTS_CONFIG = {
    "qbtc_core": {"base_url": "http://localhost:4602", "timeout": 10},
    "srona_api": {"base_url": "http://localhost:4601", "timeout": 15},
    "frontend_api": {"base_url": "http://localhost:4603", "timeout": 8},
    "vigo_futures": {"base_url": "http://localhost:8002", "timeout": 12},
    "binance_api": {"base_url": "https://api.binance.com", "timeout": 20}
}

class OrquestadorDatosOptimo:
    def __init__(self, db_path: str = "datos_optimos.db"):
        self.db_path = db_path
        self.session = None
        self.is_running = False
        self.captured_data_count = 0
        self.error_count = 0
        self.start_time = None
        
        self.init_database()
        
        logger.info("=" * 80)
        logger.info("üöÄ ORQUESTADOR DE DATOS √ìPTIMO - INICIANDO")
        logger.info("=" * 80)
        logger.info(f"üìä Base de datos: {self.db_path}")
        logger.info(f"üéØ Requerimientos de datos: {len(DATA_REQUIREMENTS)}")
        logger.info(f"‚öõÔ∏è Constantes f√≠sicas: {len(PHYSICAL_CONSTANTS)}")
        
    def init_database(self):
        """Inicializa la base de datos SQLite."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Tabla principal de datos capturados
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS captured_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    symbol TEXT,
                    data_type TEXT NOT NULL,
                    source TEXT NOT NULL,
                    timestamp DATETIME NOT NULL,
                    priority INTEGER NOT NULL,
                    data_hash TEXT UNIQUE NOT NULL,
                    data_json TEXT NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Tabla de m√©tricas de captura
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS capture_metrics (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    data_type TEXT NOT NULL,
                    success_count INTEGER DEFAULT 0,
                    error_count INTEGER DEFAULT 0,
                    last_success DATETIME,
                    last_error DATETIME,
                    avg_response_time REAL DEFAULT 0,
                    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Tabla de logs de errores
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS error_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    source TEXT NOT NULL,
                    error_type TEXT NOT NULL,
                    error_message TEXT NOT NULL,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.commit()
            conn.close()
            
            logger.info("‚úÖ Base de datos inicializada correctamente")
            
        except Exception as e:
            logger.error(f"‚ùå Error inicializando base de datos: {e}")
            raise
    
    async def start(self):
        """Inicia el orquestador de datos."""
        self.is_running = True
        self.start_time = datetime.now()
        
        logger.info("üöÄ Iniciando orquestador de datos...")
        
        timeout = aiohttp.ClientTimeout(total=30)
        self.session = aiohttp.ClientSession(timeout=timeout)
        
        try:
            await self.run_data_capture()
        except KeyboardInterrupt:
            logger.info("üõë Detenci√≥n solicitada por el usuario")
        except Exception as e:
            logger.error(f"‚ùå Error en el orquestador: {e}")
        finally:
            await self.cleanup()
    
    async def run_data_capture(self):
        """Ejecuta la captura de datos en paralelo seg√∫n las prioridades."""
        logger.info("üì° Iniciando captura de datos...")
        
        while self.is_running:
            start_cycle = time.time()
            
            # Agrupar requerimientos por prioridad
            critical_tasks = [req for req in DATA_REQUIREMENTS if req.priority == DataPriority.CRITICAL]
            high_tasks = [req for req in DATA_REQUIREMENTS if req.priority == DataPriority.HIGH]
            medium_tasks = [req for req in DATA_REQUIREMENTS if req.priority == DataPriority.MEDIUM]
            low_tasks = [req for req in DATA_REQUIREMENTS if req.priority == DataPriority.LOW]
            
            # Ejecutar captura por prioridad
            tasks = []
            
            # CR√çTICO - Ejecutar inmediatamente
            for req in critical_tasks:
                if self.should_capture(req):
                    tasks.append(self.capture_data(req))
            
            # ALTO - Ejecutar en paralelo
            high_tasks_to_capture = [req for req in high_tasks if self.should_capture(req)]
            if high_tasks_to_capture:
                tasks.extend([self.capture_data(req) for req in high_tasks_to_capture])
            
            # MEDIO - Ejecutar si hay recursos disponibles
            medium_tasks_to_capture = [req for req in medium_tasks if self.should_capture(req)]
            if medium_tasks_to_capture and len(tasks) < 10:
                tasks.extend([self.capture_data(req) for req in medium_tasks_to_capture])
            
            # BAJO - Ejecutar solo si no hay tareas cr√≠ticas
            if not tasks:
                low_tasks_to_capture = [req for req in low_tasks if self.should_capture(req)]
                if low_tasks_to_capture:
                    tasks.extend([self.capture_data(req) for req in low_tasks_to_capture])
            
            # Ejecutar todas las tareas en paralelo
            if tasks:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        self.error_count += 1
                        logger.error(f"‚ùå Error en captura: {result}")
                    else:
                        self.captured_data_count += 1
            
            # Mostrar estad√≠sticas
            cycle_time = time.time() - start_cycle
            self.log_statistics(cycle_time)
            
            # Esperar antes del siguiente ciclo
            await asyncio.sleep(10)
    
    def should_capture(self, requirement: DataRequirement) -> bool:
        """Determina si se debe capturar datos para un requerimiento espec√≠fico."""
        if not requirement.is_active:
            return False
        
        if requirement.last_captured is None:
            return True
        
        time_since_last = datetime.now() - requirement.last_captured
        return time_since_last.total_seconds() >= requirement.frequency_seconds
    
    async def capture_data(self, requirement: DataRequirement):
        """Captura datos de un endpoint espec√≠fico."""
        start_time = time.time()
        
        try:
            # Obtener configuraci√≥n del endpoint
            source_config = ENDPOINTS_CONFIG.get("qbtc_core", {})
            base_url = source_config.get("base_url", "http://localhost:4602")
            timeout = source_config.get("timeout", 10)
            
            url = f"{base_url}{requirement.endpoint}"
            
            # Realizar request HTTP
            async with self.session.get(url, timeout=timeout) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Procesar y almacenar datos
                    await self.store_data(requirement, data)
                    
                    # Actualizar m√©tricas
                    response_time = time.time() - start_time
                    self.update_metrics(requirement, True, response_time)
                    
                    requirement.last_captured = datetime.now()
                    
                    logger.info(f"‚úÖ {requirement.name}: Datos capturados ({response_time:.2f}s)")
                    return True
                    
                else:
                    raise Exception(f"HTTP {response.status}")
                    
        except Exception as e:
            self.update_metrics(requirement, False, 0)
            logger.warning(f"‚ö†Ô∏è {requirement.name}: Error: {e}")
            await self.log_error("qbtc_core", str(type(e).__name__), str(e))
            return False
    
    async def store_data(self, requirement: DataRequirement, raw_data: Dict[str, Any]):
        """Almacena los datos capturados en la base de datos."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Generar hash √∫nico para los datos
            data_str = json.dumps(raw_data, sort_keys=True)
            data_hash = str(hash(data_str))
            
            # Extraer s√≠mbolo si est√° disponible
            symbol = raw_data.get('symbol', 'GLOBAL')
            if isinstance(symbol, list) and len(symbol) > 0:
                symbol = symbol[0]
            
            # Verificar si los datos ya existen
            cursor.execute(
                "SELECT id FROM captured_data WHERE data_hash = ?",
                (data_hash,)
            )
            
            if cursor.fetchone() is None:
                # Insertar nuevos datos
                cursor.execute('''
                    INSERT INTO captured_data 
                    (symbol, data_type, source, timestamp, priority, data_hash, data_json)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    symbol,
                    requirement.name,
                    "qbtc_core",
                    datetime.now().isoformat(),
                    requirement.priority.value,
                    data_hash,
                    json.dumps(raw_data)
                ))
                
                conn.commit()
                logger.debug(f"üíæ Datos almacenados: {requirement.name}")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"‚ùå Error almacenando datos: {e}")
    
    def update_metrics(self, requirement: DataRequirement, success: bool, response_time: float):
        """Actualiza las m√©tricas de captura."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            if success:
                cursor.execute('''
                    INSERT OR REPLACE INTO capture_metrics 
                    (source, data_type, success_count, last_success, avg_response_time, updated_at)
                    VALUES (?, ?, 
                        COALESCE((SELECT success_count FROM capture_metrics WHERE source = ? AND data_type = ?), 0) + 1,
                        ?, 
                        COALESCE((SELECT avg_response_time FROM capture_metrics WHERE source = ? AND data_type = ?), 0) * 0.9 + ? * 0.1,
                        ?
                    )
                ''', (
                    "qbtc_core", requirement.name, "qbtc_core", requirement.name,
                    datetime.now().isoformat(), "qbtc_core", requirement.name,
                    response_time, datetime.now().isoformat()
                ))
            else:
                cursor.execute('''
                    INSERT OR REPLACE INTO capture_metrics 
                    (source, data_type, error_count, last_error, updated_at)
                    VALUES (?, ?, 
                        COALESCE((SELECT error_count FROM capture_metrics WHERE source = ? AND data_type = ?), 0) + 1,
                        ?, ?
                    )
                ''', (
                    "qbtc_core", requirement.name, "qbtc_core", requirement.name,
                    datetime.now().isoformat(), datetime.now().isoformat()
                ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"‚ùå Error actualizando m√©tricas: {e}")
    
    async def log_error(self, source: str, error_type: str, error_message: str):
        """Registra errores en la base de datos."""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO error_logs (source, error_type, error_message)
                VALUES (?, ?, ?)
            ''', (source, error_type, error_message))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            logger.error(f"‚ùå Error registrando error: {e}")
    
    def log_statistics(self, cycle_time: float):
        """Muestra estad√≠sticas del orquestador."""
        if self.start_time:
            uptime = datetime.now() - self.start_time
            uptime_str = str(uptime).split('.')[0]
            
            logger.info("=" * 60)
            logger.info(f"üìä ESTAD√çSTICAS DEL ORQUESTADOR")
            logger.info(f"‚è±Ô∏è  Uptime: {uptime_str}")
            logger.info(f"üìà Datos capturados: {self.captured_data_count}")
            logger.info(f"‚ùå Errores: {self.error_count}")
            logger.info(f"üîÑ Tiempo de ciclo: {cycle_time:.2f}s")
            logger.info(f"‚ö° Tasa de captura: {self.captured_data_count / max(uptime.total_seconds(), 1):.2f} datos/s")
            logger.info("=" * 60)
    
    async def cleanup(self):
        """Limpia recursos al finalizar."""
        logger.info("üßπ Limpiando recursos...")
        
        if self.session:
            await self.session.close()
        
        self.is_running = False
        logger.info("‚úÖ Orquestador detenido correctamente")

async def main():
    """Funci√≥n principal del orquestador."""
    print("=" * 80)
    print("üöÄ ORQUESTADOR DE DATOS √ìPTIMO - SISTEMA UNIFICADO QBTC")
    print("=" * 80)
    print("üìä Automatizaci√≥n completa de captura de datos")
    print("‚öõÔ∏è Backend en Python con base de datos SQLite")
    print("üéØ Basado en an√°lisis completo del sistema")
    print("=" * 80)
    
    orquestador = OrquestadorDatosOptimo()
    
    try:
        await orquestador.start()
    except KeyboardInterrupt:
        print("\nüõë Detenci√≥n solicitada por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error fatal: {e}")
        logger.error(f"Error fatal en el orquestador: {e}")

if __name__ == "__main__":
    asyncio.run(main())
